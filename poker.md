Here's an abstract example of how this works, using the game of poker. Let's say you have a couple different strategies for how you want to bet in [no-limit Texas hold-em poker](https://en.wikipedia.org/wiki/Texas_hold_%27em). One strategy goes all in if you have a pair of kings, another bets half your chips. You're not sure which strategy is best. Your objective, defined over the course of 50 hands, is to have a high probability of ending with more money than when you started. You know what two cards you hold in each hand, you also know what are people are betting, and might keep track of how they've bet in previous hands. What you bet, influences the other players behavior, and vice-versa. There is feedback in the system.

The problem of testing your strategies is complicated, for various reasons: you are uncertain about what cards the other players have been dealt, and further they have their own betting strategies (including bluffing you into thinking they have better cards than they do). The other players' strategies may even change through the course of 50 hands, depending on how many chips they have at the time any one hand is dealt. You can never know the true state of nature at a poker table while you're betting. You can, however, imagine a set of hypothetical playing styles you're likely to encounter. 

A management strategy evaluation for this poker example would take: (i) your profit objective; (ii) a mathematical model, including the basics of the game, like how the cards are dealt each hand (a good assumption would be that they are dealt randomly); (iii) your competing strategies for how to bet on a pair of kings; (iv) a set of alternative trials representing uncertainties about how other players play (the bluffer, etc.); and (v) running a large number of simulations of 50 hands of poker, for each competing strategy, against each trial. Given the summary stats and visualizations from these simulations across trials, you could make an informed judgement about which strategy is more robust than the other in terms of meeting your profit objective. 

This process would also help you identify weakness in a given strategy. You might accept that going all in with a pair of kings is a better strategy for most of the scenarios you considered, but fails in select circumstances (e.g. playing against people that never bet unless they have a pair of aces). Those are the possibilities you need to keep in mind as potentially ruinous to your strategy. So when you sit down at a game and put your strategy into action, you need to be reading the situation at the table (focused on collecting the right kind of data), and on the look-out for someone else who's playing in such a way that could break your bank. So, management strategy evaluation is also about data collection strategies. 

In fact, data collection strategies are an easy to overlook aspect of MSE. But you can't have an management strategy without a data collection strategy. So, MSE's are also useful in terms of answering questions about how to best allocate effort towards data collection when resources are limited, as is often the case in fisheries science. Another important note: __It's not a valid strategy if it completely ignores available data.__ You could propose a strategy where you go all-in on every hand no matter what cards you have, but we're not interested in evaluating that type of strategy here. At the minimum, a strategy has to at least look at the cards it's been dealt. It can, however, ignore certain aspects of those cards if it chooses (e.g. it could not pay attention to the suits on the kings). 

During different points in the hand,   
